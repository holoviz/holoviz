{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Introduction to the `.plot` API and associated libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('../data/earthquakes.parq')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using default `.plot`\n",
    "\n",
    "The first thing that we'd like to do with this data is visualize the locations of every earthquake. So we would like to make a `scatter` plot where `x='longitude'` and `y='latitude'`. If you are familiar with the `pandas.plot` API you might expect to execute: `df.plot.scatter(x='longitude', y='latitude')`. Feel free to try this out in a new cell. It throws an error: `\n",
    "AttributeError: 'DataFrame' object has no attribute 'plot'`. Since we have a dask dataframe rather than a pandas dataframe, we need to first convert it to pandas. In order to make the data more manageable, we'll use just a fraction (1%) of it and call that `small_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.sample(frac=.01).compute()\n",
    "small_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a smaller dataset with just 21k earthquakes. We can use that to test out our visualizations before ramping back up to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.plot.scatter(x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a good place to start and you can start to see the structure of the edges of the plates (which often correspond with the edges of the continents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "import hvplot.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.hvplot.scatter(x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.hvplot.scatter(x='longitude', y='latitude', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use the original full dask dataset (remember that this has 100x the points of the small dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvplot.scatter(x='longitude', y='latitude', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you zoom into the plot, the image re-renders. This can be slow (tens of seconds) when the data is being read from disk. If your computer has sufficient RAM, then you can persist the dataset in memory and significantly speed up the time that it takes to re-render the plot on zoom events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persisted.hvplot.scatter(x='longitude', y='latitude', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude\n",
    "\n",
    "Next we'll look at the frequency of different magnitude earthquakes.\n",
    "\n",
    "| Magnitude     | Earthquake Effect | Estimated Number Each Year |\n",
    "|---------------|-------------------|----------------------------|\n",
    "| 2.5 or less   | Usually not felt, but can be recorded by seismograph. |900,000|\n",
    "| 2.5 to 5.4    | Often felt, but only causes minor damage. |30,000 |\n",
    "| 5.5 to 6.0    | Slight damage to buildings and other structures. |500 |\n",
    "| 6.1 to 6.9    | May cause a lot of damage in very populated areas. | 100 |\n",
    "| 7.0 to 7.9    | Major earthquake. Serious damage. | 20 |\n",
    "| 8.0 or greater| Great earthquake. Can totally destroy communities near the epicenter. | One every 5 to 10 years |\n",
    "\n",
    "As a first pass, we'll use a histogram first with `plot.hist` on the small data, then with `.hvplot.hist` on the full dataset. Before plotting we can clean the data by setting any magnitudes that are less than 0 to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df = small_df.copy()\n",
    "cleaned_small_df['mag'] = small_df.mag.where(small_df.mag > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.mag.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.copy()\n",
    "cleaned_df['mag'] = df.mag.where(df.mag > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that seemed really fast it's because the values haven't been computed yet. All that has happened is the task graph has been set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_persisted_df = cleaned_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_persisted_df.hvplot.hist(y='mag', bin_range=(0,10), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_persisted_df.hvplot.kde(y='mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of earthquakes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.plot(x='time', y='mag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.hvplot.scatter(x='time', y='mag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.hvplot.scatter(x='time', y='mag', datashade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_persisted_df.hvplot.scatter(x='time', y='mag', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reindexed_df = cleaned_df.set_index(cleaned_df.time)\n",
    "persisted_cleaned_reindexed_df = cleaned_reindexed_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_count = persisted_cleaned_reindexed_df.id.resample('1M').count()\n",
    "monthly_count_plot = monthly_count.hvplot(title='Monthly count')\n",
    "monthly_count_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(monthly_count_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_mean_magnitude = persisted_cleaned_reindexed_df.mag.resample('1M').mean()\n",
    "monthly_mean_magnitude_plot = monthly_mean_magnitude.hvplot(title='Monthly mean magnitude')\n",
    "monthly_mean_magnitude_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(monthly_mean_magnitude_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(monthly_mean_magnitude_plot + monthly_count_plot).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More statistical plots\n",
    "\n",
    "Depth measure the depth of the event in kilometers. Some background on **depth**:\n",
    "\n",
    "> The depth where the earthquake begins to rupture. This depth may be relative to the WGS84 geoid, mean sea-level, or the average elevation of the seismic stations which provided arrival-time data for the earthquake location. The choice of reference depth is dependent on the method used to locate the earthquake, which varies by seismic network. Since ComCat includes data from many different seismic networks, the process for determining the depth is different for different events. The depth is the least-constrained parameter in the earthquake location, and the error bars are generally larger than the variation due to different depth determination methods.\n",
    "\n",
    "> Sometimes when depth is poorly constrained by available seismic data, the location program will set the depth at a fixed value. For example, 33 km is often used as a default depth for earthquakes determined to be shallow, but whose depth is not satisfactorily determined by the data, whereas default depths of 5 or 10 km are often used in mid-continental areas and on mid-ocean ridges since earthquakes in these areas are usually shallower than 33 km.\n",
    "\n",
    "Read about all the different variables available in this dataset [here](https://earthquake.usgs.gov/data/comcat/data-eventterms.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.hvplot.kde('depth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_small_df.hvplot.bivariate(x='mag', y='depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying by depth\n",
    "\n",
    "Shallow earthquakes are between 0 and 70 km deep; \n",
    "intermediate earthquakes, 70 - 300 km deep; and deep earthquakes, \n",
    "300 - 700 km deep. \n",
    "In general, the term \"deep-focus earthquakes\" is applied to earthquakes deeper than 70 km. \n",
    "All earthquakes deeper than 70 km are localized within great slabs of lithosphere that are sinking into the Earth's mantle.\n",
    "\n",
    "From https://earthquake.usgs.gov/learn/topics/determining_depth.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-np.inf, 70, 300, np.inf]\n",
    "names = ['Shallow', 'Intermediate', 'Deep']\n",
    "\n",
    "small_df['depth_class'] = pd.cut(small_df['depth'], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.hvplot.bivariate(x='mag', y='depth', groupby='depth_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df[small_df.mag >= 3].hvplot.bivariate(x='mag', y='depth', groupby='depth_class', subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying by magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Class    | Magnitude | \n",
    "|----------|-----------|\n",
    "| Great    | 8 or more | \n",
    "| Major    | 7 - 7.9   | \n",
    "| Strong   | 6 - 6.9   |\n",
    "| Moderate | 5 - 5.9   |\n",
    "| Light    | 4 - 4.9   |\n",
    "| Minor    | 3 -3.9    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = persisted[persisted.mag >= 3].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = persisted[persisted.mag >= 3].compute()\n",
    "bins = [2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 10]\n",
    "names = ['Minor', 'Light', 'Moderate', 'Strong', 'Major', 'Great']\n",
    "\n",
    "classified_df['mag_class'] = pd.cut(classified_df['mag'], bins, labels=names)\n",
    "\n",
    "bins = [-np.inf, 70, 300, np.inf]\n",
    "names = ['Shallow', 'Intermediate', 'Deep']\n",
    "\n",
    "classified_df['depth_class'] = pd.cut(classified_df['depth'], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df.hvplot.heatmap(x='mag_class', y='depth_class', C='id', reduce_function=np.count_nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a third dimension\n",
    "\n",
    "So far we have used location, time, magnitude, and depth. Now let's filter the earthquakes to only include the really high intensity ones. We can add extra dimensions to the visualization by using color in addition to x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_severe = persisted[persisted.mag >= 7].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_severe.plot.scatter(x='longitude', y='latitude', c='mag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_severe.hvplot.scatter(x='longitude', y='latitude', c='mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweaking the options to create a better plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_severe.hvplot.scatter(x='longitude', y='latitude', c='mag', hover_cols=['place', 'time'],\n",
    "                           cmap='fire_r', title='Earthquakes with magnitude >= 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that would be a lot better with a map underneath it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datashader.geo\n",
    "\n",
    "from holoviews.element.tiles import EsriImagery, Wikipedia, OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datashader.geo.lnglat_to_meters(most_severe.longitude, most_severe.latitude)\n",
    "most_severe_projected = most_severe.join([pd.DataFrame({'x': x}), pd.DataFrame({'y': y})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSM() * most_severe_projected.hvplot.scatter(x='x', y='y', c='mag', hover_cols=['place', 'time'], \n",
    "                                             cmap='fire_r', title='Earthquakes with magnitude >= 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlay with another dataset\n",
    "**Note:** that we'll need rasterio if we want to do this section. But it is available on defaults, so doesn't require conda-forge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_rasterio('../data/gpw_v4_population_density_rev11_2010_2pt5_min.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds = ds.where(ds.values != ds.nodatavals[0]).sel(band=1)\n",
    "cleaned_ds.name = 'population'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xarray.plot()` API is fine for plotting small sections of this dataset, but it doesn't do well with the full resolution. For that it is better to use `hvplot` to take advantage of datashading. Here we are plotting Indonesia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = cleaned_ds.sel(y=slice(10, -10), x=slice(90, 110))\n",
    "ROI.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ds.hvplot.image(cmap='viridis', rasterize=True, height=600, width=1000) *\\\n",
    "most_severe.hvplot.scatter(x='longitude', y='latitude', c='mag', hover_cols=['place', 'time'], cmap='fire_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries using this interface: pandas `.plot`, xarray `.plot`?, quick intro to cufflinks, hvplot.\n",
    "\n",
    "Quick easy way to generate simple plots.\n",
    "\n",
    "#### Exercise 3: Add some visualizations via the `.plot` API to the dashboard elements built in exercise 2.\n",
    "\n",
    "#### 3b: Link to `HoloViews` from `hvplot` output ('shortcuts not dead ends')\n",
    "\n",
    "Deconstruct hvplot output to show what individual elements are. Show more involved repr with `NdOverlay`. Discuss that these are are compositional objects. Show `DynamicMap` output from `hvplot`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
